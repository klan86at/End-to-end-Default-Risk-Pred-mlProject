{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5225afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bce34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a26c6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the current working directory to the project root\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ded22f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72de34a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataIngestionConfig:\n",
    "    root_dir: Path\n",
    "    source_data_file: Path\n",
    "    local_data_file: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81089b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from defaultMlProj.constants.constant import *\n",
    "from defaultMlProj.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601dfa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "    ):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_ingestion_config(self) -> DataIngestionConfig:\n",
    "        config = self.config.data_ingestion\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_ingestion_config = DataIngestionConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            source_data_file=config.source_data_file,\n",
    "            local_data_file=config.local_data_file\n",
    "        )\n",
    "        return data_ingestion_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70f1a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from defaultMlProj import logger\n",
    "from defaultMlProj.utils.common import get_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfc6262",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    def __init__ (self, config: DataIngestionConfig):\n",
    "        self.config = config\n",
    "\n",
    "    def copy_data_file(self):\n",
    "\n",
    "        source = Path(self.config.source_data_file)\n",
    "        destination = Path(self.config.local_data_file)\n",
    "\n",
    "        try:\n",
    "            logger.info(f\"Starting data ingestion:copying{source} to {destination}\")\n",
    "\n",
    "            destination.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            if not source.exists():\n",
    "                raise Exception(f\"Source file {source.absolute()} does not exist\")\n",
    "\n",
    "            if destination.exists():\n",
    "                logger.info(f\"File destination {destination} already exists. Skipping copy.\")\n",
    "            else:\n",
    "                shutil.copy(source, destination)\n",
    "                logger.info(f\"File copied successfully: {source} to {destination}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error occurred while copying data file: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f96d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Define the expected path\n",
    "source_path = Path(\"notebook/data/default.csv\")\n",
    "\n",
    "print(\"Current working directory:\", Path(\".\").absolute())\n",
    "print(\"Expected source path:\", source_path.absolute())\n",
    "print(\"Does file exist?\", source_path.exists())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bbb478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating the pipeline item on the workflow list\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_ingestion_config = config.get_data_ingestion_config()\n",
    "    data_ingestion = DataIngestion(config=data_ingestion_config)\n",
    "    data_ingestion.copy_data_file()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b15113f",
   "metadata": {},
   "source": [
    "##### Stage two Data Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223bd55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bacc249",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29517b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e19d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05920099",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9f0de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"artifacts/data_ingestion/default.csv\", sep=\"\\t\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8ddc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d27eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5bd69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataValidationConfig:\n",
    "    root_dir: Path\n",
    "    STATUS_FILE: str\n",
    "    all_schema: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2596ae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from defaultMlProj.constants.constant import *\n",
    "from defaultMlProj.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2016e268",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH\n",
    "    ):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_validation_config(self) -> DataValidationConfig:\n",
    "        config = self.config.data_validation\n",
    "        schema = self.schema.columns\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_validation_config = DataValidationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            STATUS_FILE=config.STATUS_FILE,\n",
    "            \n",
    "        )\n",
    "\n",
    "        return data_validation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db82293",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from defaultMlProj import logger\n",
    "from defaultMlProj.entity.config_entity import DataValidationConfig\n",
    "import pandas as pd\n",
    "\n",
    "class DataValidation:\n",
    "    def __init__ (self, config: DataValidationConfig):\n",
    "        self.config =config\n",
    "\n",
    "    def validate_all_columns(self) -> bool:\n",
    "        try:\n",
    "            logger.info(\"Starting data validation: validating all columns\")\n",
    "            validation_status = None\n",
    "\n",
    "            df = pd.read_csv(self.config.root_dir)\n",
    "            all_cols = list(df.columns)\n",
    "\n",
    "            all_schema = self.config.all_schema.keys()\n",
    "\n",
    "            for col in all_cols:\n",
    "                if col in all_schema:\n",
    "                    validation_status = False\n",
    "                    with open(self.config.STATUS_FILE, 'w') as f:\n",
    "                        f.write(f\"Validation status: {validation_status}\\n\")\n",
    "                else:\n",
    "                    validation_status = True\n",
    "                    with open(self.config.STATUS_FILE, 'w') as f:\n",
    "                        f.write(f\"Validation status: {validation_status}\\n\")\n",
    "            logger.info(f\"Data validation completed with status: {validation_status}\")\n",
    "\n",
    "            return validation_status\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error occurred during data validation: {e}\")\n",
    "            raise e\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c326e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline creation\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_validation_config = config.get_data_validation_config()\n",
    "    data_validation = DataValidation(config=data_validation_config)\n",
    "    data_validation.validate_all_columns()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8966d9e",
   "metadata": {},
   "source": [
    "#### Note: Validation stage skipped in my defaultMlProj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8179e0b",
   "metadata": {},
   "source": [
    "##### Workflows\n",
    "1. Update config.yaml\n",
    "2. Update schema.yaml\n",
    "3. Update params.yaml\n",
    "4. Update entity\n",
    "5. Update the configuration manager in src config\n",
    "6. Update the components\n",
    "7. Update the pipeline\n",
    "8. Update the main.py\n",
    "9. Update the app.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eadb700",
   "metadata": {},
   "source": [
    "#### Model transformation stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36cc87a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformatonConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a1d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from defaultMlProj.constants.constant import *\n",
    "from defaultMlProj.utils.common import read_yaml, create_directories\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "    ):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformatonConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "        \n",
    "        data_transformation_config = DataTransformatonConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57e01b4",
   "metadata": {},
   "source": [
    "#### Model Transformation stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a383f45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135f2d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fe0e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d458c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429fdef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567ff684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from defaultMlProj.constants.constant import *\n",
    "from defaultMlProj.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44961782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "    ):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path\n",
    "        )\n",
    "\n",
    "        return data_transformation_config\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e957f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# components/data_transformation.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from defaultMlProj import logger\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config: DataTransformationConfig):\n",
    "        self.config = config\n",
    "        self.target_column = \"default_risk_score\"  # actual target column name\n",
    "\n",
    "\n",
    "    def train_test_split(self):\n",
    "        df = pd.read_csv(self.config.data_path, sep='\\t')\n",
    "\n",
    "        try:\n",
    "            logger.info(\"Starting data transformation: train-test split\")\n",
    "            logger.info(f\"Full dataset shape: {df.shape}\")\n",
    "\n",
    "            # Validate target column exists\n",
    "            if self.target_column not in df.columns:\n",
    "                raise ValueError(f\"Target column '{self.target_column}' not found in data. Columns: {list(df.columns)}\")\n",
    "\n",
    "            # Separate features and target\n",
    "            X = df.drop(columns=[self.target_column])\n",
    "            y = df[self.target_column]\n",
    "\n",
    "            logger.info(f\"Feature matrix X shape: {X.shape}\")  # Should be (800, 9)\n",
    "            logger.info(f\"Target vector y shape: {y.shape}\")   # Should be (800,)\n",
    "\n",
    "            # Perform train-test split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(\n",
    "                X, y,\n",
    "                test_size=0.2,\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "            logger.info(f\"Train features shape: {X_train.shape}, Train target shape: {y_train.shape}\")\n",
    "            logger.info(f\"Test features shape: {X_test.shape}, Test target shape: {y_test.shape}\")\n",
    "\n",
    "            # Recombine for saving (optional: keeps target in dataset)\n",
    "            train_df = pd.DataFrame(X_train, columns=X.columns)\n",
    "            train_df[self.target_column] = y_train.values\n",
    "\n",
    "            test_df = pd.DataFrame(X_test, columns=X.columns)\n",
    "            test_df[self.target_column] = y_test.values\n",
    "\n",
    "            # Save to CSV\n",
    "            train_csv_path = os.path.join(self.config.root_dir, \"train.csv\")\n",
    "            test_csv_path = os.path.join(self.config.root_dir, \"test.csv\")\n",
    "\n",
    "            train_df.to_csv(train_csv_path, index=False)\n",
    "            test_df.to_csv(test_csv_path, index=False)\n",
    "\n",
    "            logger.info(f\"Train dataset saved to {train_csv_path}\")\n",
    "            logger.info(f\"Test dataset saved to {test_csv_path}\")\n",
    "\n",
    "            return train_df, test_df\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error occurred during train-test split: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0842fc2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline creation\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    data_transformation.train_test_split()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6898fe5f",
   "metadata": {},
   "source": [
    "#### Model Trainer Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1f547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d883c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964373f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27999a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc5de48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelTrainerConfig:\n",
    "    root_dir: Path\n",
    "    train_data_path: Path\n",
    "    test_data_path: Path\n",
    "    model_name: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f77211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from defaultMlProj.constants.constant import *\n",
    "from defaultMlProj.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5e13c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "    ):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_trainer_config(self) -> ModelTrainerConfig:\n",
    "        config = self.config.model_trainer\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_trainer_config = ModelTrainerConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            train_data_path=config.train_data_path,\n",
    "            test_data_path=config.test_data_path,\n",
    "            model_name=config.model_name,\n",
    "        )\n",
    "\n",
    "        return model_trainer_config\n",
    "    \n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Returns the parameters loaded from params.yaml\n",
    "        \"\"\"\n",
    "        return self.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a64798",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import xgboost\n",
    "from defaultMlProj import logger\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.ensemble import StackingRegressor    \n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab46c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    def __init__(self, config: ModelTrainerConfig, params):\n",
    "        self.config = config\n",
    "        self.params = params\n",
    "        self.target_column = params.target_column\n",
    "\n",
    "    def create_model(self):\n",
    "        # from sklearn.pipeline import Pipeline\n",
    "        # from sklearn.preprocessing import StandardScaler\n",
    "        # from sklearn.linear_model import LinearRegression\n",
    "        # from sklearn.neighbors import KNeighborsRegressor\n",
    "        # from sklearn.tree import DecisionTreeRegressor\n",
    "        # from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "        try:\n",
    "            logger.info(\"Started creating models\")\n",
    "            # Extract params\n",
    "            p = self.params.model_params\n",
    "\n",
    "            models = {}\n",
    "\n",
    "            # Linear Regression\n",
    "            models['LinearRegression'] = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('regressor', LinearRegression(\n",
    "                    fit_intercept=p.linear_regression.fit_intercept\n",
    "                ))\n",
    "            ])\n",
    "\n",
    "            # KNN\n",
    "            models['KNN'] = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('regressor', KNeighborsRegressor(\n",
    "                    n_neighbors=p.knn.n_neighbors,\n",
    "                    weights=p.knn.weights,\n",
    "                    algorithm=p.knn.algorithm\n",
    "                ))\n",
    "            ])\n",
    "\n",
    "            # Decision Tree\n",
    "            models['DecisionTree'] = DecisionTreeRegressor(\n",
    "                criterion=p.decision_tree.criterion,\n",
    "                max_depth=p.decision_tree.max_depth,\n",
    "                min_samples_split=p.decision_tree.min_samples_split,\n",
    "                min_samples_leaf=p.decision_tree.min_samples_leaf,\n",
    "                random_state=p.decision_tree.random_state\n",
    "            )\n",
    "            \n",
    "            # Random Forest\n",
    "            models['RandomForest'] = RandomForestRegressor(\n",
    "                n_estimators=p.random_forest.n_estimators,\n",
    "                criterion=p.random_forest.criterion,\n",
    "                max_depth=p.random_forest.max_depth,\n",
    "                min_samples_split=p.random_forest.min_samples_split,\n",
    "                min_samples_leaf=p.random_forest.min_samples_leaf,\n",
    "                random_state=p.random_forest.random_state\n",
    "            )\n",
    "\n",
    "            # Stacking Regressor\n",
    "            base_estimators = list(models.items())\n",
    "\n",
    "            final_estimator = LinearRegression(\n",
    "                fit_intercept=p.linear_regression.fit_intercept\n",
    "            )\n",
    "\n",
    "            stacking = StackingRegressor(\n",
    "                estimators=base_estimators,\n",
    "                final_estimator=final_estimator,\n",
    "                cv=p.stacking_regressor.cv,\n",
    "                n_jobs=p.stacking_regressor.n_jobs\n",
    "            )\n",
    "\n",
    "            models['Stacking Regressor'] = stacking\n",
    "            logger.info(f\"Models created: {list(models.keys())}\")\n",
    "            return models\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error occurred while creating models: {e}\")\n",
    "            raise e\n",
    "        \n",
    "    def train_and_evaluate(self):\n",
    "        logger.info(\"Starting model training with external parameters\")\n",
    "        try:\n",
    "            # Load data\n",
    "            train_df = pd.read_csv(self.config.train_data_path, sep=',')\n",
    "            test_df = pd.read_csv(self.config.test_data_path, sep=',')\n",
    "            \n",
    "            logger.info(f\"Train data shape: {train_df.shape}, Test data shape: {test_df.shape}\")\n",
    "\n",
    "            X_train = train_df.drop(columns=[self.target_column])\n",
    "            y_train = train_df[self.target_column]\n",
    "            X_test = test_df.drop(columns=[self.target_column])\n",
    "            y_test = test_df[self.target_column]\n",
    "\n",
    "            logger.info(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "            logger.info(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "            # Create models using params\n",
    "            models = self.create_model()\n",
    "\n",
    "            # Get CV settings from params\n",
    "            cv_params = self.params.cv_settings\n",
    "            cv = KFold(\n",
    "                n_splits=cv_params.n_splits,\n",
    "                shuffle=cv_params.shuffle,\n",
    "                random_state=cv_params.random_state\n",
    "            )\n",
    "            results = {}\n",
    "\n",
    "            for name, model in models.items():\n",
    "                try:\n",
    "                    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='r2')\n",
    "                    results[name] = scores\n",
    "                    logger.info(f\"{name} R2 = {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "                except Exception as e:\n",
    "                    logger.exception(f\"Failed to evaluate {name}: {e}\")\n",
    "                    raise e\n",
    "                \n",
    "            # The best model\n",
    "            best_name = max(results, key=lambda k: results[k].mean())\n",
    "            best_model = models[best_name].fit(X_train, y_train)\n",
    "\n",
    "            # Final evaluation\n",
    "            y_pred = best_model.predict(X_test)\n",
    "            test_r2 = r2_score(y_test, y_pred)\n",
    "            test_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "            logger.info(f\"Best model: {best_name} | Test R2 : {test_r2:.4f}, RMSE : {test_rmse:.4f}\")\n",
    "\n",
    "            # Save model\n",
    "            Path(self.config.model_name).parent.mkdir(parents=True, exist_ok=True)\n",
    "            joblib.dump(best_model, self.config.model_name)\n",
    "            logger.info(f\"Model saved to {self.config.model_name}\")\n",
    "\n",
    "            return best_model, test_r2, test_rmse\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error occurred during model training and evaluation: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23e920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline creation\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_trainer_config = config.get_model_trainer_config()\n",
    "    params = config.get_params()\n",
    "    model_trainer = ModelTrainer(config=model_trainer_config, params=params)\n",
    "    model_trainer.train_and_evaluate()\n",
    "except Exception as e:\n",
    "    logger.info(f\"Error in model training pipeline: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc8e039",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(model_trainer_config.train_data_path, sep='\\t')\n",
    "print(\"Columns in train_df:\", train_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4610a304",
   "metadata": {},
   "source": [
    "#### Model Evaluation Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c486280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3191a3ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\End-to-end-Default-Risk-Pred-mlProject\\\\notebook'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b206650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1af91934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\End-to-end-Default-Risk-Pred-mlProject'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fac37dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-25 15:26:53,581: INFO: _client: HTTP Request: GET https://dagshub.com/api/v1/user \"HTTP/1.1 200 OK\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as klan86at\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Accessing as klan86at\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-25 15:26:53,590: INFO: helpers: Accessing as klan86at]\n",
      "[2025-07-25 15:26:54,444: INFO: _client: HTTP Request: GET https://dagshub.com/api/v1/repos/klan86at/Default-risk-prediction \"HTTP/1.1 200 OK\"]\n",
      "[2025-07-25 15:26:55,325: INFO: _client: HTTP Request: GET https://dagshub.com/api/v1/user \"HTTP/1.1 200 OK\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"klan86at/Default-risk-prediction\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"klan86at/Default-risk-prediction\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-25 15:26:55,335: INFO: helpers: Initialized MLflow to track repo \"klan86at/Default-risk-prediction\"]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository klan86at/Default-risk-prediction initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository klan86at/Default-risk-prediction initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-25 15:26:55,339: INFO: helpers: Repository klan86at/Default-risk-prediction initialized!]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/25 15:26:55 INFO mlflow.tracking.fluent: Experiment with name 'DefaultRiskPrediction' does not exist. Creating a new experiment.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='mlflow-artifacts:/73d4320aaa634e8498ebadec4f5a8af9', creation_time=1753446414716, experiment_id='0', last_update_time=1753446414716, lifecycle_stage='active', name='DefaultRiskPrediction', tags={}>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure DagsHub for MLflow tracking\n",
    "import dagshub\n",
    "import mlflow\n",
    "\n",
    "dagshub.init(repo_owner='klan86at', repo_name='Default-risk-prediction', mlflow=True)\n",
    "\n",
    "# Now MLflow is connected to DagsHub\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/klan86at/Default-risk-prediction.mlflow\")\n",
    "mlflow.set_experiment(\"DefaultRiskPrediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e3296da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    test_data_path: Path\n",
    "    model_path: Path\n",
    "    metric_file_name: str\n",
    "    mlflow_uri: str\n",
    "    experiment_name: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b659c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from defaultMlProj.constants.constant import *\n",
    "from defaultMlProj.utils.common import read_yaml, create_directories, save_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6c37f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelEvaluation Configuration Manager\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "    ):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            test_data_path=config.test_data_path,\n",
    "            model_path=config.model_path,\n",
    "            metric_file_name=config.metric_file_name,\n",
    "            mlflow_uri=config.mlflow_uri,\n",
    "            experiment_name=config.experiment_name\n",
    "        )\n",
    "        \n",
    "        return model_evaluation_config\n",
    "    \n",
    "    def get_params(self):\n",
    "        \"\"\"\n",
    "        Returns the parameters loaded from params.yaml\n",
    "        \"\"\"\n",
    "        return self.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b018cd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation component\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from defaultMlProj import logger\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "from defaultMlProj.entity.config_entity import ModelEvaluationConfig\n",
    "from defaultMlProj.utils.common import read_yaml, create_directories\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig, params):\n",
    "        self.config = config\n",
    "        self.params = params\n",
    "        self.target_column = params.target_column\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        try:\n",
    "            logger.info(\"Starting model evaluation with Mlflow logging\")\n",
    "\n",
    "            os.makedirs(self.config.root_dir, exist_ok=True)\n",
    "\n",
    "            # Load test data\n",
    "            test_df = pd.read_csv(self.config.test_data_path, sep=',')\n",
    "            X_test = test_df.drop(columns=[self.target_column])\n",
    "            y_test = test_df[self.target_column]\n",
    "\n",
    "            logger.info(f\"Test data shape: {test_df.shape}\")\n",
    "\n",
    "            # Load trained model\n",
    "            model = joblib.load(self.config.model_path)\n",
    "            logger.info(f\"Model loaded from {self.config.model_path}\")\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_test)\n",
    "\n",
    "            # Metrics\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            rmse = mean_squared_error(y_test, y_pred)\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "            # Save metrics to JSON\n",
    "            metrics = {\n",
    "                \"r2_score\": r2,\n",
    "                \"rmse\": rmse,\n",
    "                \"mae\": mae\n",
    "            }\n",
    "\n",
    "            with open(self.config.metric_file_name, 'w') as f:\n",
    "                json.dump(metrics, f, indent=4)\n",
    "\n",
    "            logger.info(f\"Metrics saved to {self.config.metric_file_name}\")\n",
    "\n",
    "            # Set up MLflow\n",
    "            mlflow.set_tracking_uri(self.config.mlflow_uri)\n",
    "            mlflow.set_experiment(self.config.experiment_name)\n",
    "\n",
    "            with mlflow.start_run():\n",
    "                # Log the parameters\n",
    "                self.log_params_flattened(self.params.model_params)\n",
    "                mlflow.log_param(\"target_column\", self.target_column)\n",
    "                mlflow.log_param(\"cv_splits\", self.params.cv_settings.n_splits)\n",
    "                \n",
    "                # Log the metrics\n",
    "                mlflow.log_metric(\"r2_score\", r2)\n",
    "                mlflow.log_metric(\"rmse\", rmse)\n",
    "                mlflow.log_metric(\"mae\", mae)\n",
    "\n",
    "                # Saving model\n",
    "                model_temp_path = Path(self.config.model_path)\n",
    "                model_temp_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                joblib.dump(model, model_temp_path)\n",
    "                mlflow.log_artifact(model_temp_path, \"model\")\n",
    "                logger.info(f\"Model logged to MLflow as artifact: {model_temp_path}\")\n",
    "                \n",
    "\n",
    "                # Log artifacts\n",
    "                mlflow.log_artifact(self.config.metric_file_name)\n",
    "                logger.info(f\"Model and metrics logged to mlflow under experiment '{self.config.experiment_name}'\")\n",
    "\n",
    "            return metrics\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error occurred during model evaluation: {e}\")\n",
    "            raise e\n",
    "        \n",
    "    def log_params_flattened(self, params, parent_key=''):\n",
    "        \"\"\" Recursively log parameters to MLflow, flattening nested dictionaries.\n",
    "        \"\"\"\n",
    "        for key, value in params.items():\n",
    "            new_key = f\"{parent_key}.{key}\" if parent_key else key\n",
    "            if isinstance(value, dict):\n",
    "                    self.log_params_flattened(value, new_key)\n",
    "            else:\n",
    "                mlflow.log_param(new_key, value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3ad53525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-07-25 15:47:19,010: INFO: common: YAML file config\\config.yaml loaded successfully.]\n",
      "[2025-07-25 15:47:19,016: INFO: common: YAML file params.yaml loaded successfully.]\n",
      "[2025-07-25 15:47:19,018: INFO: common: Created directory: artifacts]\n",
      "[2025-07-25 15:47:19,021: INFO: common: Created directory: artifacts/model_evaluation]\n",
      "[2025-07-25 15:47:19,022: INFO: 2561630473: Starting model evaluation with Mlflow logging]\n",
      "[2025-07-25 15:47:19,029: INFO: 2561630473: Test data shape: (160, 10)]\n",
      "[2025-07-25 15:47:19,033: INFO: 2561630473: Model loaded from artifacts/model_trainer/model.joblib]\n",
      "[2025-07-25 15:47:19,040: INFO: 2561630473: Metrics saved to artifacts/model_evaluation/metrics.json]\n",
      "[2025-07-25 15:47:28,739: WARNING: connectionpool: Retrying (Retry(total=6, connect=7, read=6, redirect=7, status=7)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /klan86at/Default-risk-prediction.mlflow/api/2.0/mlflow/experiments/get-by-name?experiment_name=DefaultRiskPrediction]\n",
      "[2025-07-25 15:47:51,370: INFO: 2561630473: Model logged to MLflow as artifact: artifacts\\model_trainer\\model.joblib]\n",
      "[2025-07-25 15:47:52,494: INFO: 2561630473: Model and metrics logged to mlflow under experiment 'DefaultRiskPrediction']\n",
      "🏃 View run unleashed-lark-34 at: https://dagshub.com/klan86at/Default-risk-prediction.mlflow/#/experiments/0/runs/dfdb441e3bf54ab5bf3b81b993499df3\n",
      "🧪 View experiment at: https://dagshub.com/klan86at/Default-risk-prediction.mlflow/#/experiments/0\n",
      "[2025-07-25 15:47:54,193: INFO: 4017845782: Model evaluation completed with metrics: {'r2_score': 0.9999836636868409, 'rmse': 99.50785520790922, 'mae': 8.049826688023005}]\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation  pipeline\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    model_eval_config = config.get_model_evaluation_config()\n",
    "    params = config.get_params()\n",
    "    model_eval = ModelEvaluation(config=model_eval_config, params=params)\n",
    "    metrics = model_eval.evaluate_model()\n",
    "    logger.info(f\"Model evaluation completed with metrics: {metrics}\")\n",
    "except Exception as e:\n",
    "    logger.info(f\"Error in model evaluation pipeline: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c24fe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install dagshub mlflow\n",
    "\n",
    "# import dagshub\n",
    "# dagshub.init(repo_owner='klan86at',\n",
    "#              repo_name='Default-risk-prediction',\n",
    "#              mlflow=True)\n",
    "\n",
    "# import mlflow\n",
    "# with mlflow.start_run():\n",
    "#   mlflow.log_param('parameter name', 'value')\n",
    "#   mlflow.log_metric('metric name', 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
